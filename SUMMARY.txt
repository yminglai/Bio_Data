================================================================================
                    1-to-1 SAE PIPELINE SUMMARY
================================================================================

GOAL: Train 6-slot SAE where each slot = exactly one semantic rule

┌─────────────────────────────────────────────────────────────────────────┐
│                            THE 6 RULES                                   │
├─────────────────────────────────────────────────────────────────────────┤
│  Slot 0: Birth Date    →  "What is {NAME}'s birth date?"               │
│  Slot 1: Birth City    →  "Where was {NAME} born?"                     │
│  Slot 2: University    →  "Which university did {NAME} attend?"        │
│  Slot 3: Major         →  "What did {NAME} study?"                     │
│  Slot 4: Employer      →  "Who does {NAME} work for?"                  │
│  Slot 5: Work City     →  "Where does {NAME} work?"                    │
└─────────────────────────────────────────────────────────────────────────┘

================================================================================
                         PIPELINE STEPS
================================================================================

Step 1: Generate Dataset
├─ 1000 synthetic persons (500 train, 500 test - DISJOINT)
├─ 4 question templates per rule (1 train, 3 test)
├─ Template split ensures OOD evaluation
└─ Output: train_kg.json, test_kg.json, qa_*.jsonl

Step 2: Fine-tune Base LLM
├─ Light SFT on biography + QA
├─ Ensures consistent answering
└─ Output: models/base_sft/final/

Step 3: Collect Activations
├─ Extract hidden states at answer position
├─ From final layer residual stream
└─ Output: train_activations.pkl

Step 4: Train 6-Slot SAE
├─ Supervised slot assignment (Gumbel-Softmax)
├─ Multi-objective loss (recon + sparse + align + indep + value)
├─ Temperature annealing: 1.0 → 0.1
└─ Output: sae_final.pt

Step 5: Evaluate Binding Accuracy ⭐
├─ Question → Relation binding (slot activation)
├─ Relation → Answer binding (answer generation)
├─ OOD generalization test (unseen templates)
└─ Output: binding_accuracy_results.json + visualizations

================================================================================
                      THE CRITICAL TEST: OOD BINDING
================================================================================

Does the SAE learn SEMANTIC CONCEPTS or just MEMORIZE TEMPLATES?

Example: Birth Date Rule

┌──────────────────────────────────────────────────────────────────────┐
│ TRAINING (Template 0)                                                 │
│   "What is Alice Johnson's birth date?"                              │
│   → Slot 0 activates ✓                                               │
└──────────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────────┐
│ TEST-OOD (Template 2 - NEVER SEEN!)                                  │
│   "On what date was Alice Johnson born?"  ← Different phrasing!     │
│                                                                       │
│   ✅ IF SLOT 0 ACTIVATES: Learned semantic concept                   │
│   ❌ IF RANDOM SLOT:      Just memorized template                    │
└──────────────────────────────────────────────────────────────────────┘

================================================================================
                        SUCCESS CRITERIA
================================================================================

Metric                          Target    Meaning
────────────────────────────────────────────────────────────────────────
Train Slot Binding              ≥ 0.95    Model learned training data
Test-ID Slot Binding            ≥ 0.85    Generalizes to new persons
Test-OOD Slot Binding ⭐         ≥ 0.75    Generalizes to new phrasings!
Diagonal Accuracy               ≥ 0.85    True 1-to-1 mapping
Per-Rule OOD Accuracy           ≥ 0.70    All rules work

================================================================================
                        EXAMPLE RESULTS
================================================================================

✅ SUCCESSFUL 1-TO-1 SAE:

  Question → Relation Binding (Slot Activation):
    Train:    0.98  ✅
    Test-ID:  0.92  ✅
    Test-OOD: 0.87  ✅  ← Generalizes to unseen phrasings!

  Relation → Answer Binding (Answer Generation):
    Train:    0.95  ✅
    Test-ID:  0.89  ✅
    Test-OOD: 0.82  ✅

  Diagonal Accuracy: 0.94  ✅

  Per-Rule OOD Slot Binding:
    birth_date : 0.92  ✅
    birth_city : 0.89  ✅
    university : 0.85  ✅
    major      : 0.84  ✅
    employer   : 0.88  ✅
    work_city  : 0.81  ✅

  Confusion Matrix (should be diagonal):
                   bd    bc    uni   maj   emp   wc
    Slot 0 (bd)   0.95  0.02  0.01  0.01  0.00  0.01
    Slot 1 (bc)   0.02  0.93  0.00  0.02  0.01  0.02
    Slot 2 (uni)  0.01  0.01  0.94  0.02  0.01  0.01
    Slot 3 (maj)  0.01  0.02  0.02  0.92  0.02  0.01
    Slot 4 (emp)  0.00  0.01  0.01  0.02  0.94  0.01
    Slot 5 (wc)   0.01  0.01  0.02  0.01  0.02  0.94

  ✨ TRUE 1-TO-1 MAPPING ACHIEVED! ✨

================================================================================
                        HOW TO RUN
================================================================================

Full Pipeline:
  $ python run_pipeline.py

Individual Steps:
  $ python scripts/01_generate_dataset.py
  $ python scripts/02_sft_base_model.py
  $ python scripts/03_collect_activations.py
  $ python scripts/04_train_sae.py
  $ python scripts/05_evaluate_sae.py

Quick Test (small dataset):
  $ bash quick_test.sh

================================================================================
                        KEY FILES
================================================================================

Code:
  scripts/01_generate_dataset.py    - Data generation
  scripts/02_sft_base_model.py      - Base LLM fine-tuning
  scripts/03_collect_activations.py - Activation collection
  scripts/04_train_sae.py           - SAE training ⭐
  scripts/05_evaluate_sae.py        - Binding accuracy eval ⭐

Documentation:
  README.md                         - Quick overview
  PIPELINE_README.md                - Complete guide
  BINDING_ACCURACY.md               - Binding accuracy explained
  OOD_TEMPLATE_TEST.md              - OOD test explained ⭐

Data:
  data/qa_templates/                - Question templates (4 per rule)
  data/entities/                    - Names, cities, etc.
  data/generated/                   - Generated KG and QA pairs

Results:
  models/sae_6slot/sae_final.pt     - Trained SAE
  results/sae_eval/                 - Evaluation results

================================================================================
                     WHY THIS MATTERS
================================================================================

Traditional SAE:
  ❌ Many latents, unclear meaning
  ❌ No guarantee of interpretability
  ❌ Hard to verify what each feature does

1-to-1 SAE with Binding Accuracy:
  ✅ Exactly 6 latents, one per rule
  ✅ Verified interpretability via OOD test
  ✅ Each feature = clear semantic concept
  ✅ Steerable: edit features to control model
  ✅ Robust: works on unseen phrasings

This enables:
  • Feature-based model steering
  • Interpretable mechanistic understanding
  • Reliable intervention and debugging
  • Causal knowledge editing

================================================================================
